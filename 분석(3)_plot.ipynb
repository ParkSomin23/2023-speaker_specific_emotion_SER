{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import yaml, itertools\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "import my_utils\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import os\n",
    "import wandb\n",
    "\n",
    "import glob\n",
    "import numpy as np\n",
    "import utils_data_10Fold\n",
    "import utils_sampler\n",
    "import copy\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_list(emo_list):\n",
    "    color = {'neu':'slategrey', 'ang':'crimson', 'hap':'gold', \n",
    "             'sad':'darkblue',  'dis':'plum', \n",
    "             'sur':'steelblue', 'fea':'olivedrab'}\n",
    "    \n",
    "    color_list = []\n",
    "    for e in emo_list:\n",
    "        c = color[e[:3]]\n",
    "        color_list.append(c)\n",
    "        \n",
    "    return color_list\n",
    "\n",
    "def make_center_color(color_list):\n",
    "    color = {'slategrey':'black', 'crimson':'orangered', 'gold':'yellow', \n",
    "             'darkblue':'blue',  'plum':'purple', \n",
    "             'steelblue':'dodgerblue', 'olivedrab':'darkolivegreen'}\n",
    "    \n",
    "    center_list = []\n",
    "    for e in color_list:\n",
    "        c = color[e]\n",
    "        center_list.append(c)\n",
    "        \n",
    "    return center_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IEMOCAP_Dataset(Dataset):\n",
    "    def __init__(self, data, hparams):\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.csv = data\n",
    "        self.output_class = hparams['use_class']\n",
    "        self.vad = hparams['vad']\n",
    "        self.sr = 16000\n",
    "\n",
    "        self.aranged_id_num = list(range(1,11))\n",
    "        self.aranged_id_num.remove(2*hparams['fold_num']-1)\n",
    "        self.aranged_id_num.remove(2*hparams['fold_num'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = self.csv.iloc[idx]\n",
    "        ids = data['session']\n",
    "        ans = self.output_class.index(data['emotion'])\n",
    "\n",
    "        try:\n",
    "            pid = self.aranged_id_num.index(data['id_num'])\n",
    "        except:\n",
    "            if data['id_num'] % 2 == 0:\n",
    "                pid = 8\n",
    "            else:\n",
    "                pid = 9\n",
    "\n",
    "        if self.vad:\n",
    "            wav_path = '/media/ubuntu/SSD2/Dataset/IEMOCAP_VAD/Session'+str(data['fold'])\\\n",
    "                        +'/' + '_'.join(ids.split('_')[:-1]) + '/' + ids +'.wav'\n",
    "        else:\n",
    "            wav_path = '/media/ubuntu/SSD2/Dataset/IEMOCAP_full_release/Session'+str(data['fold'])\\\n",
    "                         +'/sentences/wav/' + '_'.join(ids.split('_')[:-1]) + '/' + ids +'.wav'\n",
    "            if os.getcwd().split('/')[2] == 'cvnar2':\n",
    "                wav_path = wav_path.replace('/media/ubuntu/SSD2/Dataset', '/home/cvnar2/Desktop/ssd')\n",
    "\n",
    "\n",
    "        wav, sr = torchaudio.load(wav_path, normalize=True)\n",
    "        if self.hparams['repeat_3_sec']:\n",
    "            if wav.shape[-1] / self.sr < 3.0:\n",
    "                n_repeat = int(3 // (wav.shape[-1] / self.sr))\n",
    "                wav = wav.repeat((1, n_repeat))\n",
    "                \n",
    "        if (data['sec'] > self.hparams['max_sec']):\n",
    "            max_len = int(16000 * self.hparams['max_sec'])\n",
    "            offset = random.randint(0, wav.shape[1] - max_len - 1)\n",
    "            wav = wav[:, offset:offset+max_len]\n",
    "\n",
    "        inputs = wav.transpose(0, 1).squeeze(1)\n",
    "        \n",
    "        return inputs, ans, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = '/home/ubuntu/Dropbox/23_for_revision/with_emotion/Vox_IEMO/only_emo_hs'#'Vox_IEMO(3)/main_final2_load_emo_beta/all_finetune_beta(05)'\n",
    "exp_list = os.listdir(main_path)\n",
    "exp_list.sort()\n",
    "\n",
    "bz = 1\n",
    "if os.path.isdir(main_path + '/plot'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(main_path + '/plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_230215_2208',\n",
       " '2_230215_2347',\n",
       " '3_230216_0117',\n",
       " '4_230216_0258',\n",
       " '5_230216_0425',\n",
       " 'replot']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1초 skipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "class IEMOCAP_Test_Dataset(IEMOCAP_Dataset):\n",
    "    def __init__(self, data, hparams):\n",
    "        super(IEMOCAP_Test_Dataset, self).__init__(data, hparams)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = self.csv.iloc[idx]\n",
    "        ids = data['session']\n",
    "        ans = self.output_class.index(data['emotion'])\n",
    "\n",
    "        try:\n",
    "            pid = self.aranged_id_num.index(data['id_num'])\n",
    "        except:\n",
    "            if data['id_num'] % 2 == 0:\n",
    "                pid = 8\n",
    "            else:\n",
    "                pid = 9\n",
    "\n",
    "        if self.vad:\n",
    "            wav_path = '/media/ubuntu/SSD2/Dataset/IEMOCAP_VAD/Session'+str(data['fold'])\\\n",
    "                        +'/' + '_'.join(ids.split('_')[:-1]) + '/' + ids +'.wav'\n",
    "        else:\n",
    "            wav_path = '/media/ubuntu/SSD2/Dataset/IEMOCAP_full_release/Session'+str(data['fold'])\\\n",
    "                         +'/sentences/wav/' + '_'.join(ids.split('_')[:-1]) + '/' + ids +'.wav'\n",
    "            if os.getcwd().split('/')[2] == 'cvnar2':\n",
    "                wav_path = wav_path.replace('/media/ubuntu/SSD2/Dataset', '/home/cvnar2/Desktop/ssd')\n",
    "\n",
    "\n",
    "        wav, sr = torchaudio.load(wav_path, normalize=True)\n",
    "        if wav.shape[-1] < int(self.sr * self.hparams['max_sec']):\n",
    "            n = 1\n",
    "            inputs = wav.transpose(0, 1).squeeze(1)\n",
    "\n",
    "        elif wav.shape[-1] > int(self.sr * self.hparams['max_sec']):\n",
    "            inputs = wav.squeeze(0).unfold(0, int(self.sr*self.hparams['max_sec']), self.sr)\n",
    "            n = len(inputs)\n",
    "\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "        ans = [ans] * n\n",
    "        pid = [pid] * n\n",
    "        \n",
    "        return inputs, ans, pid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_acc(output, answer, balanced_accuracy):\n",
    "\n",
    "    pred = np.array(output).reshape(-1,1)\n",
    "    answ = np.array(answer).reshape(-1,1)\n",
    "\n",
    "    cm = confusion_matrix(answ, pred)\n",
    "    if balanced_accuracy:\n",
    "        acc = balanced_accuracy_score(answ, pred)\n",
    "    else:\n",
    "        acc = (np.eye(len(cm)) * cm).sum() / cm.sum()\n",
    "\n",
    "    return acc\n",
    "\n",
    "def eval_network(model, data_loader, device):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    i = 0\n",
    "    emo_answ_dict = []\n",
    "    emo_pred_dict = []\n",
    "\n",
    "    for inputs, ans, pid in tqdm(data_loader):\n",
    "        inputs = inputs.to(device=device)\n",
    "        if len(inputs.shape) >= 3:\n",
    "            inputs = inputs.squeeze(0)\n",
    "        \n",
    "        ans = torch.Tensor(ans).to(device=device)\n",
    "        pid = torch.Tensor(pid).to(device=device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            out, feat, feat_list = model.get_feat(inputs)\n",
    "\n",
    "            p = out.clone().detach().cpu().mean(dim=0).argmax(dim=-1).reshape(-1).numpy()[0]\n",
    "            a = ans[0].clone().detach().cpu().reshape(-1).numpy()[0]\n",
    "\n",
    "            emo_answ_dict.append(a)\n",
    "            emo_pred_dict.append(p)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    acc_dict = {'id_UA' :[], 'id_WA':[],\n",
    "                'emo_UA':[], 'emo_WA':[]}\n",
    "    \n",
    "    emo_answ_dict = np.array(emo_answ_dict)\n",
    "    emo_pred_dict = np.array(emo_pred_dict)\n",
    "\n",
    "    acc_dict['emo_UA'] = get_pred_acc(emo_pred_dict, emo_answ_dict, balanced_accuracy=True)\n",
    "    acc_dict['emo_WA'] = get_pred_acc(emo_pred_dict, emo_answ_dict, balanced_accuracy=False)\n",
    "\n",
    "    #v_acc = acc_dict['emo_UA'] \n",
    "    #print(v_acc)\n",
    "    \n",
    "    return acc_dict, [emo_answ_dict, emo_pred_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = 'best'\n",
    "for exp in exp_list:\n",
    "    if exp == 'plot':\n",
    "        continue\n",
    "    print(exp)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    lib_path = glob.glob(main_path+'/'+exp+'/*main*.py')[0][:-3].replace('/', '.')\n",
    "    saved_main = importlib.import_module(lib_path)\n",
    "\n",
    "    with open(main_path+'/'+exp+\"/hparams.yaml\") as f:\n",
    "        hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    seed = hparams['seed']\n",
    "    my_utils.set_seed(seed)\n",
    "\n",
    "    net = saved_main.Emotion_Network(hparams)\n",
    "    if hparams['id_net_freeze'] != 'freeze':\n",
    "        net.id_net.hs.fc = nn.Parameter(torch.Tensor(8, hparams['fin_channel']))\n",
    "\n",
    "    weight = torch.load(main_path+'/'+exp+\"/\"+mode+\"_model.pt\")\n",
    "    missing_keys = net.load_state_dict(weight['model_state_dict'], strict=True)\n",
    "    print(missing_keys)\n",
    "\n",
    "    net = net.cuda()\n",
    "    net.eval()\n",
    "\n",
    "    emo_list = hparams['use_class']\n",
    "    \n",
    "    trainset, validset, testset = utils_data_10Fold.real_5Fold(hparams)\n",
    "    \n",
    "    test_set = IEMOCAP_Test_Dataset(testset, hparams)\n",
    "    testloader = DataLoader(test_set, shuffle=True, batch_size=bz, collate_fn=saved_main.Pad_Collate, pin_memory=True)\n",
    "\n",
    "    id = '_'.join([hparams['emo_hs_linear'], exp])\n",
    "    wandb.init(project=hparams['pj_name'], name=id, id=id, resume=True) #, config = hparams)\n",
    "    # acc_dict, [emo_answ_dict, emo_pred_dict]\n",
    "    test_acc_dict, [test_emo_answ_dict, test_emo_pred_dict] = eval_network(net, testloader, 'cuda')\n",
    "\n",
    "    wandb.run.summary[\"test_\"+mode+\"_emo_WA\"] = test_acc_dict['emo_WA']\n",
    "    wandb.run.summary[\"test_\"+mode+\"_emo_UA\"] = test_acc_dict['emo_UA']\n",
    "    t2 = test_acc_dict['emo_UA']\n",
    "\n",
    "    fin_print = f'[emo acc: {t2:.3f}'\n",
    "    print(fin_print)\n",
    "\n",
    "    wandb.log({\"test_\"+mode+\"_cm\" : wandb.plot.confusion_matrix(\n",
    "                y_true=test_emo_answ_dict, preds=test_emo_pred_dict,\n",
    "                class_names=emo_list)})\n",
    "\n",
    "    wandb.finish()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# plot T-sne"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_230215_2208\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/ubuntu/Desktop/2023/final2_hyperparams/분석(3)_plot.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ubuntu/Desktop/2023/final2_hyperparams/%EB%B6%84%EC%84%9D%283%29_plot.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(exp)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ubuntu/Desktop/2023/final2_hyperparams/%EB%B6%84%EC%84%9D%283%29_plot.ipynb#X14sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ubuntu/Desktop/2023/final2_hyperparams/%EB%B6%84%EC%84%9D%283%29_plot.ipynb#X14sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m lib_path \u001b[39m=\u001b[39m glob\u001b[39m.\u001b[39;49mglob(main_path\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m+\u001b[39;49mexp\u001b[39m+\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m/*main*.py\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m0\u001b[39;49m][:\u001b[39m-\u001b[39m\u001b[39m3\u001b[39m]\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntu/Desktop/2023/final2_hyperparams/%EB%B6%84%EC%84%9D%283%29_plot.ipynb#X14sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m saved_main \u001b[39m=\u001b[39m importlib\u001b[39m.\u001b[39mimport_module(lib_path)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ubuntu/Desktop/2023/final2_hyperparams/%EB%B6%84%EC%84%9D%283%29_plot.ipynb#X14sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(main_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39m+\u001b[39mexp\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m/hparams.yaml\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m f:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "## emo: plot id: label \n",
    "for exp in exp_list:\n",
    "    if exp == 'plot':\n",
    "        continue\n",
    "    print(exp)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    lib_path = glob.glob(main_path+'/'+exp+'/*main*.py')[0][:-3].replace('/', '.')\n",
    "    saved_main = importlib.import_module(lib_path)\n",
    "\n",
    "    with open(main_path+'/'+exp+\"/hparams.yaml\") as f:\n",
    "        hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    seed = hparams['seed']\n",
    "    my_utils.set_seed(seed)\n",
    "\n",
    "    print(hparams['repeat_3_sec'])\n",
    "\n",
    "    net = saved_main.Emotion_Network(hparams)\n",
    "    if hparams['id_net_freeze'] != 'freeze':\n",
    "        net.id_net.hs.fc = nn.Parameter(torch.Tensor(8, hparams['fin_channel']))\n",
    "\n",
    "    net.id_filter.data = nn.Parameter(torch.Tensor(1, hparams['pool_head']+1, hparams['fin_channel']))\n",
    "\n",
    "    weight = torch.load(main_path+'/'+exp+\"/best_model.pt\")\n",
    "    missing_keys = net.load_state_dict(weight['model_state_dict'], strict=True)\n",
    "    print(missing_keys)\n",
    "\n",
    "    net = net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    trainset, validset, testset = utils_data_10Fold.real_5Fold(hparams)\n",
    "    \n",
    "    test_set = IEMOCAP_Dataset(testset, hparams)\n",
    "    testloader = DataLoader(test_set, shuffle=True, batch_size=bz, collate_fn=saved_main.Pad_Collate, pin_memory=True)\n",
    "\n",
    "    valid_set = IEMOCAP_Dataset(validset, hparams)\n",
    "    validloader = DataLoader(valid_set, shuffle=True, batch_size=bz, collate_fn=saved_main.Pad_Collate, pin_memory=True)\n",
    "\n",
    "    trainset = IEMOCAP_Dataset(trainset, hparams)\n",
    "    trainloader = DataLoader(trainset, batch_size=bz, collate_fn=saved_main.Pad_Collate, pin_memory=True)\n",
    "\n",
    "    # GE FEATURES\n",
    "    full_inp = []\n",
    "    emo_feat_list = []\n",
    "    id_feat_list = []\n",
    "    feat_list = []\n",
    "    new_feat_list = []\n",
    "\n",
    "    id_list = []\n",
    "    ans_list = []\n",
    "    loader_list = []\n",
    "\n",
    "    emo_list = hparams['use_class']\n",
    "    color_list = make_color_list(emo_list)\n",
    "\n",
    "    for idx, iemo_dataloader in enumerate([trainloader, validloader, testloader]):\n",
    "\n",
    "        if idx == 0:\n",
    "            name = \"train\"\n",
    "        elif idx == 1:\n",
    "            name = \"valid\"\n",
    "        else:\n",
    "            name = \"test\"\n",
    "\n",
    "        for wav, ans, pid in tqdm(iemo_dataloader):\n",
    "            wav = wav.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out, feat, feat_list = net.get_feat(wav)\n",
    "\n",
    "            emo_feat_list.extend(feat.detach().cpu().numpy())\n",
    "            id_feat_list.extend(feat_list[0].detach().cpu().numpy())\n",
    "            #feat_list.extend(feat.detach().cpu().numpy())\n",
    "            #new_feat_list.extend(new_feat.detach().cpu().numpy())\n",
    "\n",
    "            id_list.append(int(pid[0][0].item()))\n",
    "            ans_list.append(int(ans[0][0].item()))\n",
    "            loader_list.append(name)\n",
    "\n",
    "    id_list = np.array(id_list)\n",
    "    ans_list = np.array(ans_list)\n",
    "    loader_list = np.array(loader_list)\n",
    "\n",
    "    emo_feat_list = np.array(emo_feat_list)\n",
    "    id_feat_list = np.array(id_feat_list)\n",
    "    #feat_list = np.array(feat_list)\n",
    "    #new_feat_list = np.array(new_feat_list)\n",
    "    \n",
    "    # PLOT\n",
    "    tsne = TSNE(n_components=2, perplexity=20, init='pca', learning_rate='auto')\n",
    "    y_tsne = tsne.fit_transform(emo_feat_list)\n",
    "\n",
    "    # EMOTION\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(16,10))\n",
    "    for n_idx, name in enumerate(['train', 'valid', 'test']):\n",
    "        mask1 = np.where(loader_list==name, True, False)\n",
    "        i = 0\n",
    "        for k in list(set(ans_list)):\n",
    "            mask2 = np.where(ans_list==k, True, False)\n",
    "\n",
    "            mask = mask1 & mask2\n",
    "\n",
    "            x = y_tsne[mask][:, 0]\n",
    "            y = y_tsne[mask][:, 1]\n",
    "            ax[0][n_idx].scatter(x, y, label=emo_list[i], color=color_list[i], alpha=0.5, s=10)\n",
    "            i+=1\n",
    "\n",
    "        ax[0][n_idx].legend()\n",
    "        ax[0][n_idx].set_title(exp + ' ' + name)\n",
    "\n",
    "    # ID\n",
    "    cmap1 = plt.cm.get_cmap('tab10', 10)\n",
    "    for n_idx, name in enumerate(['train', 'valid', 'test']):\n",
    "        mask1 = np.where(loader_list==name, True, False)\n",
    "        i = 0\n",
    "        for k in list(set(id_list)):\n",
    "            mask2 = np.where(id_list==k, True, False)\n",
    "\n",
    "            mask = mask1 & mask2\n",
    "\n",
    "            x = y_tsne[mask][:, 0]\n",
    "            y = y_tsne[mask][:, 1]\n",
    "            ax[1][n_idx].scatter(x, y, label=k, color=cmap1(i), alpha=0.5, s=10)\n",
    "            i+=1\n",
    "        \n",
    "        ax[1][n_idx].legend()\n",
    "        ax[1][n_idx].set_title(exp + ' ' + name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(main_path + '/plot/'+exp+'_emo_id_repeat3sec.png')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## emo: plot id: label \n",
    "for exp in exp_list:\n",
    "    if exp == 'plot':\n",
    "        continue\n",
    "    print(exp)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    lib_path = glob.glob(main_path+'/'+exp+'/*main*.py')[0][:-3].replace('/', '.')\n",
    "    saved_main = importlib.import_module(lib_path)\n",
    "\n",
    "    with open(main_path+'/'+exp+\"/hparams.yaml\") as f:\n",
    "        hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    seed = hparams['seed']\n",
    "    my_utils.set_seed(seed)\n",
    "\n",
    "    net = saved_main.Emotion_Network(hparams)\n",
    "    if hparams['id_net_freeze'] != 'freeze':\n",
    "        net.id_net.hs.fc = nn.Parameter(torch.Tensor(8, hparams['fin_channel']))\n",
    "\n",
    "    weight = torch.load(main_path+'/'+exp+\"/best_model.pt\")\n",
    "    missing_keys = net.load_state_dict(weight['model_state_dict'], strict=True)\n",
    "    print(missing_keys)\n",
    "\n",
    "    net = net.cuda()\n",
    "    net.eval()\n",
    "    \n",
    "    trainset, validset, testset = utils_data_10Fold.real_5Fold(hparams)\n",
    "    \n",
    "    test_set = IEMOCAP_Dataset(testset, hparams)\n",
    "    testloader = DataLoader(test_set, shuffle=True, batch_size=bz, collate_fn=saved_main.Pad_Collate, pin_memory=True)\n",
    "\n",
    "    valid_set = IEMOCAP_Dataset(validset, hparams)\n",
    "    validloader = DataLoader(valid_set, shuffle=True, batch_size=bz, collate_fn=saved_main.Pad_Collate, pin_memory=True)\n",
    "\n",
    "    trainset = IEMOCAP_Dataset(trainset, hparams)\n",
    "    trainloader = DataLoader(trainset, batch_size=bz, collate_fn=saved_main.Pad_Collate, pin_memory=True)\n",
    "\n",
    "    \"\"\"id = '_'.join([hparams['emo_hs_linear'], exp])\n",
    "    wandb.init(project=hparams['pj_name'], name=id, id=id, resume=True) #, config = hparams)\n",
    "    _, test_acc_dict, [test_emo_answ_dict, test_emo_pred_dict] = saved_main.eval_network(net, testloader, hparams)\n",
    "\n",
    "    wandb.run.summary[\"test_emo_WA\"] = test_acc_dict['emo_WA']\n",
    "    wandb.run.summary[\"test_emo_UA\"] = test_acc_dict['emo_UA']\n",
    "    t2 = test_acc_dict['emo_UA']\n",
    "\n",
    "    fin_print = f'[emo acc: {t2:.3f}'\n",
    "    print(fin_print)\n",
    "\n",
    "    wandb.log({\"test_cm\" : wandb.plot.confusion_matrix(\n",
    "                y_true=test_emo_answ_dict, preds=test_emo_pred_dict,\n",
    "                class_names=emo_list)})\n",
    "\n",
    "    wandb.finish()\"\"\"\n",
    "\n",
    "    # GE FEATURES\n",
    "    \n",
    "\n",
    "    emo_list = hparams['use_class']\n",
    "    color_list = make_color_list(emo_list)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 3, figsize=(16,10))\n",
    "    for idx, iemo_dataloader in enumerate([trainloader, validloader, testloader]):\n",
    "\n",
    "        full_inp = []\n",
    "        emo_feat_list = []\n",
    "        id_feat_list = []\n",
    "        feat_list = []\n",
    "        new_feat_list = []\n",
    "\n",
    "        id_list = []\n",
    "        ans_list = []\n",
    "        loader_list = []\n",
    "\n",
    "        if idx == 0:\n",
    "            name = \"train\"\n",
    "        elif idx == 1:\n",
    "            name = \"valid\"\n",
    "        else:\n",
    "            name = \"test\"\n",
    "\n",
    "        for wav, ans, pid in tqdm(iemo_dataloader):\n",
    "            wav = wav.cuda()\n",
    "\n",
    "            with torch.no_grad():\n",
    "                out, feat, feat_list = net.get_feat(wav)\n",
    "\n",
    "            emo_feat_list.extend(feat.detach().cpu().numpy())\n",
    "            id_feat_list.extend(feat_list[0].detach().cpu().numpy())\n",
    "            #feat_list.extend(feat.detach().cpu().numpy())\n",
    "            #new_feat_list.extend(new_feat.detach().cpu().numpy())\n",
    "\n",
    "            id_list.append(int(pid[0][0].item()))\n",
    "            ans_list.append(int(ans[0][0].item()))\n",
    "            loader_list.append(name)\n",
    "\n",
    "        id_list = np.array(id_list)\n",
    "        ans_list = np.array(ans_list)\n",
    "        loader_list = np.array(loader_list)\n",
    "\n",
    "        emo_feat_list = np.array(emo_feat_list)\n",
    "        id_feat_list = np.array(id_feat_list)\n",
    "        #feat_list = np.array(feat_list)\n",
    "        #new_feat_list = np.array(new_feat_list)\n",
    "        \n",
    "        # PLOT\n",
    "        tsne = TSNE(n_components=2, perplexity=20, init='pca', learning_rate='auto')\n",
    "        y_tsne = tsne.fit_transform(emo_feat_list)\n",
    "\n",
    "        # EMOTION\n",
    "        i = 0\n",
    "        for k in list(set(ans_list)):\n",
    "            mask = np.where(ans_list==k, True, False)\n",
    "\n",
    "            x = y_tsne[mask][:, 0]\n",
    "            y = y_tsne[mask][:, 1]\n",
    "            ax[0][idx].scatter(x, y, label=emo_list[i], color=color_list[i], alpha=0.5, s=10)\n",
    "            i+=1\n",
    "\n",
    "        ax[0][idx].legend()\n",
    "        ax[0][idx].set_title(exp + ' ' + name)\n",
    "\n",
    "        # ID\n",
    "        cmap1 = plt.cm.get_cmap('tab10', 10)\n",
    "        i = 0\n",
    "        for k in list(set(id_list)):\n",
    "            mask = np.where(id_list==k, True, False)\n",
    "\n",
    "            x = y_tsne[mask][:, 0]\n",
    "            y = y_tsne[mask][:, 1]\n",
    "            ax[1][idx].scatter(x, y, label=k, color=cmap1(i), alpha=0.5, s=10)\n",
    "            i+=1\n",
    "        \n",
    "        ax[1][idx].legend()\n",
    "        ax[1][idx].set_title(exp + ' ' + name)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(main_path + '/plot/'+exp+'_emo_id_each.png')\n",
    "    plt.show()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90029da15b0834c5efe5e94393ce3d13cd8de7e62c32e215fddda54b994b687e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
