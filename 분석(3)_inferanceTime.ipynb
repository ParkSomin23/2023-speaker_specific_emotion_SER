{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=0\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import yaml, itertools\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "15  Final Throughput: 79.69875661325088\n",
      "10  Final Throughput: 125.31364764875822\n",
      "5  Final Throughput: 247.69535615132457\n",
      "3  Final Throughput: 413.5603851008161\n"
     ]
    }
   ],
   "source": [
    "optimal_batch_size = 8\n",
    "device = 'cuda'\n",
    "\n",
    "# MIPS(Million Instructions Per Second)\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "base_w2v = bundle.get_model()\n",
    "base_w2v.to(device)\n",
    "base_w2v.eval()\n",
    "print(base_w2v.training)\n",
    "\n",
    "for sec in [15, 10, 5, 3]:\n",
    "    dummy_input = torch.randn(optimal_batch_size, int(16000*sec), dtype=torch.float).to(device)\n",
    "    repetitions=100\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _ = base_w2v(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            total_time += curr_time\n",
    "    Throughput =  (repetitions*optimal_batch_size)/total_time\n",
    "    print(sec,' Final Throughput:',Throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_large_ls960.pth\" to /home/ubuntu/.cache/torch/hub/checkpoints/wav2vec2_fairseq_large_ls960.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bfdcfdeb0b64577a2e6bc4aa47aa3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/1.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "15  Final Throughput: 34.51715504113194\n",
      "10  Final Throughput: 52.420661430209655\n",
      "5  Final Throughput: 110.9933839696896\n",
      "3  Final Throughput: 198.5234116959865\n"
     ]
    }
   ],
   "source": [
    "optimal_batch_size = 8\n",
    "device = 'cuda'\n",
    "\n",
    "# MIPS(Million Instructions Per Second)\n",
    "\n",
    "bundle = torchaudio.pipelines.WAV2VEC2_LARGE\n",
    "base_w2v = bundle.get_model()\n",
    "base_w2v.to(device)\n",
    "base_w2v.eval()\n",
    "print(base_w2v.training)\n",
    "\n",
    "for sec in [15, 10, 5, 3]:\n",
    "    dummy_input = torch.randn(optimal_batch_size, int(16000*sec), dtype=torch.float).to(device)\n",
    "    repetitions=100\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _ = base_w2v(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            total_time += curr_time\n",
    "    Throughput =  (repetitions*optimal_batch_size)/total_time\n",
    "    print(sec,' Final Throughput:',Throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_color_list(emo_list):\n",
    "    color = {'neu':'slategrey', 'ang':'crimson', 'hap':'gold', \n",
    "             'sad':'darkblue',  'dis':'plum', \n",
    "             'sur':'steelblue', 'fea':'olivedrab'}\n",
    "    \n",
    "    color_list = []\n",
    "    for e in emo_list:\n",
    "        c = color[e[:3]]\n",
    "        color_list.append(c)\n",
    "        \n",
    "    return color_list\n",
    "\n",
    "def make_center_color(color_list):\n",
    "    color = {'slategrey':'black', 'crimson':'orangered', 'gold':'yellow', \n",
    "             'darkblue':'blue',  'plum':'purple', \n",
    "             'steelblue':'dodgerblue', 'olivedrab':'darkolivegreen'}\n",
    "    \n",
    "    center_list = []\n",
    "    for e in color_list:\n",
    "        c = color[e]\n",
    "        center_list.append(c)\n",
    "        \n",
    "    return center_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IEMOCAP_Dataset(Dataset):\n",
    "    def __init__(self, data, hparams):\n",
    "        \n",
    "        self.hparams = hparams\n",
    "        self.csv = data\n",
    "        self.output_class = hparams['use_class']\n",
    "        self.vad = hparams['vad']\n",
    "        self.sr = 16000\n",
    "\n",
    "        self.aranged_id_num = list(range(1,11))\n",
    "        self.aranged_id_num.remove(2*hparams['fold_num']-1)\n",
    "        self.aranged_id_num.remove(2*hparams['fold_num'])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = self.csv.iloc[idx]\n",
    "        ids = data['session']\n",
    "        ans = self.output_class.index(data['emotion'])\n",
    "\n",
    "        try:\n",
    "            pid = self.aranged_id_num.index(data['id_num'])\n",
    "        except:\n",
    "            if data['id_num'] % 2 == 0:\n",
    "                pid = 8\n",
    "            else:\n",
    "                pid = 9\n",
    "\n",
    "        if self.vad:\n",
    "            wav_path = '/media/ubuntu/SSD2/Dataset/IEMOCAP_VAD/Session'+str(data['fold'])\\\n",
    "                        +'/' + '_'.join(ids.split('_')[:-1]) + '/' + ids +'.wav'\n",
    "        else:\n",
    "            wav_path = '/media/ubuntu/SSD2/Dataset/IEMOCAP_full_release/Session'+str(data['fold'])\\\n",
    "                         +'/sentences/wav/' + '_'.join(ids.split('_')[:-1]) + '/' + ids +'.wav'\n",
    "            if os.getcwd().split('/')[2] == 'cvnar2':\n",
    "                wav_path = wav_path.replace('/media/ubuntu/SSD2/Dataset', '/home/cvnar2/Desktop/ssd')\n",
    "\n",
    "\n",
    "        wav, sr = torchaudio.load(wav_path, normalize=True)\n",
    "        if self.hparams['repeat_3_sec']:\n",
    "            if wav.shape[-1] / self.sr < 3.0:\n",
    "                n_repeat = int(3 // (wav.shape[-1] / self.sr))\n",
    "                wav = wav.repeat((1, n_repeat))\n",
    "                \n",
    "        if (data['sec'] > self.hparams['max_sec']):\n",
    "            max_len = int(16000 * self.hparams['max_sec'])\n",
    "            offset = random.randint(0, wav.shape[1] - max_len - 1)\n",
    "            wav = wav[:, offset:offset+max_len]\n",
    "\n",
    "        inputs = wav.transpose(0, 1).squeeze(1)\n",
    "        \n",
    "        return inputs, ans, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pad_Collate(samples):\n",
    "    from torch.nn.utils.rnn import pad_sequence\n",
    "    \"\"\"\n",
    "    DataLoader collate_fn\n",
    "    \"\"\"\n",
    "    #for i in range(len(samples)):\n",
    "    #    print(samples[i][0])\n",
    "    #    print()\n",
    "    inputs = [sample[0].squeeze(0) for sample in samples]  \n",
    "    padded_inputs = pad_sequence(inputs, batch_first=True)\n",
    "\n",
    "    labels = [sample[1] for sample in samples]\n",
    "    labels = torch.Tensor(labels).float()\n",
    "    labels = labels.unsqueeze(1)\n",
    "\n",
    "    ids = [sample[2] for sample in samples]\n",
    "        \n",
    "    return padded_inputs, labels, ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = 'weight/main_final2_load_emo/freeze'#'Vox_IEMO(3)/main_final2_load_emo_beta/all_finetune_beta(05)'\n",
    "exp_list = os.listdir(main_path)\n",
    "exp_list.sort()\n",
    "\n",
    "bz = 1\n",
    "if os.path.isdir(main_path + '/plot'):\n",
    "    pass\n",
    "else:\n",
    "    os.mkdir(main_path + '/plot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1_230413_0649',\n",
       " '2_230413_0800',\n",
       " '3_230413_0913',\n",
       " '4_230413_1020',\n",
       " '5_230413_1125',\n",
       " 'plot']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import loss_class\n",
    "import pool_module\n",
    "\n",
    "class ID_Network(nn.Module):\n",
    "    def __init__(self, hparams, n_class):\n",
    "        super(ID_Network, self).__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "        self.w2v = bundle.get_model()\n",
    "\n",
    "        self.pool = pool_module.AttentionalPool(hparams['fin_channel'], 4, 0.1, 'max')\n",
    "        if self.hparams['id_hs_linear'] == 'linear':\n",
    "            self.hs = nn.Linear(hparams['fin_channel'], n_class)\n",
    "        elif self.hparams['id_hs_linear'] == 'hs':\n",
    "            self.hs = loss_class.HS_Loss(n_class, hparams['id_scale'], hparams['id_margin'], hparams['fin_channel'])\n",
    "        else:\n",
    "            raise ValueError('hs_linear value error')\n",
    "        \n",
    "    def forward(self, x, ans):\n",
    "\n",
    "        batch, _ = x.size()\n",
    "        x, _ = self.w2v(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        feat = x.view(batch, -1)\n",
    "\n",
    "        if self.hparams['id_hs_linear'] == 'linear':    \n",
    "            out = self.hs(feat)\n",
    "        elif self.hparams['id_hs_linear'] == 'hs':\n",
    "            out = self.hs(feat, ans.reshape(-1).long())\n",
    "        else:\n",
    "            raise ValueError('id_hs_linear value error')\n",
    "\n",
    "        return out, ans, feat\n",
    "\n",
    "    def get_feat(self, x):\n",
    "\n",
    "        batch, _ = x.size()\n",
    "        x, _ = self.w2v(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        feat = x.view(batch, -1)\n",
    "\n",
    "        return feat\n",
    "\n",
    "    def get_close_id(self, x):\n",
    "\n",
    "        batch, _ = x.size()\n",
    "        x, _ = self.w2v(x)\n",
    "\n",
    "        x = self.pool(x)\n",
    "        feat = x.view(batch, -1)\n",
    "\n",
    "        if self.hparams['id_hs_linear'] == 'linear':    \n",
    "            out = self.hs(feat)\n",
    "\n",
    "        elif self.hparams['id_hs_linear'] == 'hs':\n",
    "            out = F.linear(F.normalize(feat), F.normalize(self.hs.fc))\n",
    "            out = out.clamp(-1, 1)\n",
    "\n",
    "        return feat, out\n",
    "\n",
    "class Emotion_Network(nn.Module):\n",
    "    def __init__(self, hparams):\n",
    "        super(Emotion_Network, self).__init__()\n",
    "\n",
    "        self.hparams = hparams\n",
    "\n",
    "        self.id_net = ID_Network(hparams, 1251)\n",
    "\n",
    "        bundle = torchaudio.pipelines.WAV2VEC2_BASE\n",
    "        self.w2v = bundle.get_model()\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(hparams['fin_channel'], hparams['fin_channel'])\n",
    "\n",
    "        self.pool_layers = [pool_module.AttentionalPool(hparams['fin_channel'], 4, 0.1, 'max') for _ in range(hparams['pool_head'])]\n",
    "        self.pool_layers = nn.ModuleList(self.pool_layers)\n",
    "\n",
    "        if self.hparams['emo_hs_linear'] == 'linear':\n",
    "            self.hs = nn.Linear(hparams['fin_channel'], 4)\n",
    "        elif self.hparams['emo_hs_linear'] == 'hs':\n",
    "            self.hs = loss_class.HS_Loss(4, hparams['emo_scale'], hparams['emo_margin'], hparams['fin_channel'])\n",
    "        else:\n",
    "            raise ValueError('emo_hs_linear value error')\n",
    "\n",
    "        self.id_filter = nn.Parameter(torch.randn(1, int(hparams['pool_head']), 768).float())\n",
    "\n",
    "    def forward(self, x, ans, ids):\n",
    "\n",
    "        batch, _ = x.size()\n",
    "        if self.hparams['id_net_freeze'] == 'freeze':\n",
    "            with torch.no_grad():\n",
    "            # id_loss, id_out, id_ans, id_feat\n",
    "                id_feat, id_out = self.id_net.get_close_id(x)\n",
    "        else:\n",
    "            # out, ans, feat\n",
    "            id_out, _, id_feat = self.id_net(x, ids)\n",
    "\n",
    "        x, _ = self.w2v(x)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        out_list = [id_feat]\n",
    "        for pool_layer in self.pool_layers:\n",
    "            tmp = pool_layer(x)\n",
    "            tmp = tmp.view(batch, -1)\n",
    "            out_list.append(tmp)\n",
    "\n",
    "        feat = torch.stack(out_list, dim=1)\n",
    "        # out = torch.stack((id_feat, n_out, a_out, h_out, s_out), dim=1)\n",
    "\n",
    "        feat = feat * self.id_filter\n",
    "        feat = feat.sum(dim=1)\n",
    "\n",
    "        if self.hparams['emo_hs_linear'] == 'linear':    \n",
    "            out = self.hs(feat)\n",
    "        elif self.hparams['emo_hs_linear'] == 'hs':\n",
    "            out = self.hs(feat, ans.reshape(-1).long())\n",
    "        else:\n",
    "            raise ValueError('emo_hs_linear value error')\n",
    "\n",
    "        return out, id_out\n",
    "    \n",
    "    def get_feat(self, x):\n",
    "        batch, _ = x.size()\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            id_feat, id_out = self.id_net.get_close_id(x) \n",
    "            x, norm_x = self.w2v.extract_features(x)\n",
    "\n",
    "            x = self.relu(x[-1])\n",
    "            x = self.fc1(x)\n",
    "\n",
    "            feat_list = [id_feat]\n",
    "            for pool_layer in self.pool_layers:\n",
    "                tmp = pool_layer(x)\n",
    "                tmp = tmp.view(batch, -1)\n",
    "                feat_list.append(tmp)\n",
    "\n",
    "            feat = torch.stack(feat_list, dim=1)\n",
    "            # out = torch.stack((id_feat, n_out, a_out, h_out, s_out), dim=1)\n",
    "\n",
    "            feat = feat * self.id_filter\n",
    "            feat = feat.sum(dim=1)\n",
    "\n",
    "            if self.hparams['emo_hs_linear'] == 'linear':    \n",
    "                out = self.hs(feat)\n",
    "            elif self.hparams['emo_hs_linear'] == 'hs':\n",
    "                out = F.linear(F.normalize(feat), F.normalize(self.hs.fc))\n",
    "                out = out.clamp(-1, 1)\n",
    "            else:\n",
    "                raise ValueError('emo_hs_linear value error')\n",
    "\n",
    "            return [out, id_out], feat, feat_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "pool_ 1\n",
      "<All keys matched successfully>\n",
      "15  Final Throughput: 77.19154586201564\n",
      "10  Final Throughput: 121.32358994750444\n",
      "5  Final Throughput: 240.50776208573978\n",
      "3  Final Throughput: 401.2057941055703\n",
      "\n",
      "pool_ 2\n",
      "<All keys matched successfully>\n",
      "15  Final Throughput: 74.54757764696272\n",
      "10  Final Throughput: 117.33124072091567\n",
      "5  Final Throughput: 233.66060438224494\n",
      "3  Final Throughput: 388.391619951946\n",
      "\n",
      "pool_ 3\n",
      "<All keys matched successfully>\n",
      "15  Final Throughput: 72.24856833525553\n",
      "10  Final Throughput: 113.88599196064663\n",
      "5  Final Throughput: 227.27003918430196\n",
      "3  Final Throughput: 378.93564673672876\n",
      "\n",
      "pool_ 4\n",
      "<All keys matched successfully>\n",
      "15  Final Throughput: 70.33185145372038\n",
      "10  Final Throughput: 111.05684636004726\n",
      "5  Final Throughput: 221.42185378578756\n",
      "3  Final Throughput: 368.89824865998315\n"
     ]
    }
   ],
   "source": [
    "import my_utils\n",
    "\n",
    "for p in [1,2,3,4]:\n",
    "    print()\n",
    "    print('pool_', p)\n",
    "    main_path = 'weight/only_emo/pool_' + str(p) #'Vox_IEMO(3)/main_final2_load_emo_beta/all_finetune_beta(05)'\n",
    "    exp_list = os.listdir(main_path)\n",
    "    exp_list.sort()\n",
    "\n",
    "    exp = exp_list[0]\n",
    "    mode = 'best'\n",
    "    device = 'cuda'\n",
    "\n",
    "    optimal_batch_size = 8 \n",
    "\n",
    "    lib_path = glob.glob(main_path+'/'+exp+'/*only_emo*.py')[0][:-3].replace('/', '.')\n",
    "    saved_main = importlib.import_module(lib_path)\n",
    "\n",
    "    u_path = glob.glob(main_path+'/'+exp+'/my_utils.py')[0][:-3].replace('/', '.')\n",
    "    my_utils = importlib.import_module(u_path)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    with open(main_path+'/'+exp+\"/hparams.yaml\") as f:\n",
    "        hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "    seed = hparams['seed']\n",
    "    my_utils.set_seed(seed)\n",
    "\n",
    "    net = saved_main.Emotion_Network(hparams)\n",
    "\n",
    "    weight = torch.load(main_path+'/'+exp+\"/\"+mode+\"_model.pt\")\n",
    "    missing_keys = net.load_state_dict(weight['model_state_dict'], strict=True)\n",
    "    print(missing_keys)\n",
    "\n",
    "    net = net.cuda()\n",
    "    net.eval()\n",
    "\n",
    "    # MIPS(Million Instructions Per Second)\n",
    "    for sec in [15, 10, 5, 3]:\n",
    "        dummy_input = torch.randn(optimal_batch_size, int(16000*sec), dtype=torch.float).to(device)\n",
    "        repetitions=100\n",
    "        total_time = 0\n",
    "        with torch.no_grad():\n",
    "            for rep in range(repetitions):\n",
    "                starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "                starter.record()\n",
    "                _, _, _ = net.get_feat(dummy_input)\n",
    "                ender.record()\n",
    "                torch.cuda.synchronize()\n",
    "                curr_time = starter.elapsed_time(ender)/1000\n",
    "                total_time += curr_time\n",
    "        Throughput =  (repetitions*optimal_batch_size)/total_time\n",
    "        print(sec,' Final Throughput:',Throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<All keys matched successfully>\n",
      "15  Final Throughput: 36.689831027403386\n",
      "10  Final Throughput: 57.458186672329624\n",
      "5  Final Throughput: 113.71012777813627\n",
      "3  Final Throughput: 189.27038607823263\n"
     ]
    }
   ],
   "source": [
    "import my_utils\n",
    "\n",
    "exp = exp_list[0]\n",
    "mode = 'best'\n",
    "device = 'cuda'\n",
    "\n",
    "optimal_batch_size = 8 \n",
    "\n",
    "lib_path = glob.glob(main_path+'/'+exp+'/*main*.py')[0][:-3].replace('/', '.')\n",
    "saved_main = importlib.import_module(lib_path)\n",
    "\n",
    "u_path = glob.glob(main_path+'/'+exp+'/my_utils.py')[0][:-3].replace('/', '.')\n",
    "my_utils = importlib.import_module(u_path)\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "with open(main_path+'/'+exp+\"/hparams.yaml\") as f:\n",
    "    hparams = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "seed = hparams['seed']\n",
    "my_utils.set_seed(seed)\n",
    "\n",
    "net = saved_main.Emotion_Network(hparams)\n",
    "\n",
    "if hparams['id_net_freeze'] is not None:\n",
    "    net.id_filter.data = torch.randn(1, hparams['pool_head']+1, hparams['fin_channel']) \n",
    "\n",
    "if hparams['id_net_freeze'] != 'freeze':\n",
    "    net.id_net.hs.fc = nn.Parameter(torch.Tensor(8, hparams['fin_channel']))\n",
    "\n",
    "weight = torch.load(main_path+'/'+exp+\"/\"+mode+\"_model.pt\")\n",
    "missing_keys = net.load_state_dict(weight['model_state_dict'], strict=True)\n",
    "print(missing_keys)\n",
    "\n",
    "net = net.cuda()\n",
    "net.eval()\n",
    "\n",
    "# MIPS(Million Instructions Per Second)\n",
    "for sec in [15, 10, 5, 3]:\n",
    "    dummy_input = torch.randn(optimal_batch_size, int(16000*sec), dtype=torch.float).to(device)\n",
    "    repetitions=100\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _, _, _ = net.get_feat(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            total_time += curr_time\n",
    "    Throughput =  (repetitions*optimal_batch_size)/total_time\n",
    "    print(sec,' Final Throughput:',Throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  Final Throughput: 76.47565492882147\n",
      "10  Final Throughput: 120.16652757372611\n",
      "5  Final Throughput: 238.34744691195974\n",
      "3  Final Throughput: 397.4481681954243\n"
     ]
    }
   ],
   "source": [
    "for sec in [15, 10, 5, 3]:\n",
    "    dummy_input = torch.randn(optimal_batch_size, int(16000*sec), dtype=torch.float).to(device)\n",
    "    repetitions=100\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _ = net.id_net.get_feat(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            total_time += curr_time\n",
    "    Throughput =  (repetitions*optimal_batch_size)/total_time\n",
    "    print(sec,' Final Throughput:',Throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15  Final Throughput: 78.65637441178347\n",
      "10  Final Throughput: 123.5405113223635\n",
      "5  Final Throughput: 244.4819497883157\n",
      "3  Final Throughput: 407.40979517856573\n"
     ]
    }
   ],
   "source": [
    "for sec in [15, 10, 5, 3]:\n",
    "    dummy_input = torch.randn(optimal_batch_size, int(16000*sec), dtype=torch.float).to(device)\n",
    "    repetitions=100\n",
    "    total_time = 0\n",
    "    with torch.no_grad():\n",
    "        for rep in range(repetitions):\n",
    "            starter, ender = torch.cuda.Event(enable_timing=True), torch.cuda.Event(enable_timing=True)\n",
    "            starter.record()\n",
    "            _, _ = net.w2v.extract_features(dummy_input)\n",
    "            ender.record()\n",
    "            torch.cuda.synchronize()\n",
    "            curr_time = starter.elapsed_time(ender)/1000\n",
    "            total_time += curr_time\n",
    "    Throughput =  (repetitions*optimal_batch_size)/total_time\n",
    "    print(sec,' Final Throughput:',Throughput)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "speechbrain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "90029da15b0834c5efe5e94393ce3d13cd8de7e62c32e215fddda54b994b687e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
